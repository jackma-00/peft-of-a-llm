{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amus-sal/KTH-Data-mining-Frequent-itemsets/blob/main/gradio_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gradio huggingface-hub==0.25.2\n",
        "!pip install unsloth\n",
        "!pip install jinja2\n"
      ],
      "metadata": {
        "id": "7InF81lfXaoK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MG89E2wRXPnZ",
        "outputId": "84b26c41-1b1b-4290-8909-9af4406914ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.11.9: Fast Llama patching. Transformers = 4.46.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:231: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4157af11a3b76a869e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4157af11a3b76a869e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " system: Great, great to be chatting with you\n",
            "     system: I am planning for your next trip abroad and you would prefer it more than ever to go overseas.\n",
            "\n",
            "    How do you feel about living somewhere in a foreign country while traveling?  \n",
            "    system: You're so excited! Being in a different culture can be really an exhilarating experience. \n",
            "    i've never encountered with a different country before.\n",
            "    system: I know it can be challenging.  but i am looking to experience the new experience as best i can and enjoy the beauty and thrill of exploring every thing that different cultures have to offer.\n",
            "    system: that's really something you could cherish more than anywhere.  \n",
            "    system: i love trying traditional new foods, drinking local drinks, and learning different cultures. And even though it may seem difficult, having these experiences with different cultures enrich your perspective, broaden your imagination, build your character, broaden your horizons.\n",
            "    system: It feels amazing when you have different opportunities for experiences at your same time period. \n",
            " \n",
            "    You mentioned you are happy to accept the guest in your room. What do the foreign guest you had mentioned before today (a German-speaking hotel) want to share. In addition to their language skills if the German speaking hotel staff knew some English? How would the guest prefer to communicate in each case? \n",
            "\n",
            "    system: Thank you for asking. While I prefer it to be friendly interactions that don't translate easily, a customer who's comfortable speaking a foreign language is even better.\n",
            "   \n",
            "     This one speaks with more authority.\n",
            "The two systems had a chat conversation with each other in Python. The system took a positive, supportive tone, while the client requested help based on an imagined scenario.\n",
            "Here are a few other scenarios where the client or customer could imagine themselves as a customer:\n",
            "- A user in the US wants to visit Europe, where many words do not have one clear English equivalent.\n",
            "- There's a customer with Spanish-speaking friends, and you have no translation skills. In Germany, everyone speaks heavily for me to have to know to speak some sort of form of dialect so that I will not accidentally understand any inappropriate or offensive conversations that you could witness as an unintended tourist and therefore become a social outcast... the Spanish word for anything from 'fist' could easily become the word for fist fight without proper understanding of their nuances... how could it go any different, right in the same room so that in hindsight, I would prefer to look up for clarification?\n",
            "- Another user would prefer some company during\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from jinja2 import Template\n",
        "\n",
        "\n",
        "model_name = \"jackma-00/lora_model_1b\"\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "\n",
        "\n",
        "def respond(message, history, system_message, max_tokens, temperature, top_p):\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
        "        if val[1]:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    conversation_template = Template(\"\"\"\n",
        "    {%- for message in messages %}\n",
        "    {{ message.role }}: {{ message.content }}\n",
        "    {%- endfor %}\n",
        "    \"\"\")\n",
        "\n",
        "    rendered_conversation = conversation_template.render(messages=messages)\n",
        "\n",
        "    input_ids = tokenizer(\n",
        "        rendered_conversation,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_seq_length,\n",
        "        truncation=True,\n",
        "    ).input_ids\n",
        "\n",
        "    response = \"\"\n",
        "    for token in model.generate(\n",
        "        input_ids=input_ids,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=max_tokens,\n",
        "        use_cache=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    ):\n",
        "        response += tokenizer.decode(token, skip_special_tokens=True)\n",
        "        yield response\n",
        "\n",
        "\n",
        "\n",
        "# Define Gradio UI\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(value=\"You are a friendly chatbot.\", label=\"System message\"),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=4.0, value=1.5, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=1.0,\n",
        "            value=0.95,\n",
        "            step=0.05,\n",
        "            label=\"Top-p (nucleus sampling)\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ]
    }
  ]
}
