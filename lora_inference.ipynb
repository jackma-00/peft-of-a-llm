{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "\n",
    "max_seq_length = 2048 # Maximum number of tokens in a sequence\n",
    "dtype = None # Data type of the model input tensor\n",
    "\n",
    "model_name = \"jackma-00/lora_model\" # Model name\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name, \n",
    "    max_seq_length=max_seq_length, \n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly Chatbot.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True, # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
